{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict,OrderedDict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import requests\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford_ner_dir = '/home/dell/stanford-ner-2015-04-20/'\n",
    "eng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        \"\"\"\n",
    "         Initilize the text summarizer.\n",
    "         Words that have a frequency term lower than min_cut \n",
    "         or higer than max_cut will be ignored.\n",
    "        \"\"\"\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        \"\"\" \n",
    "          Compute the frequency of each of word.\n",
    "          Input: \n",
    "           word_sent, a list of sentences already tokenized.\n",
    "          Output: \n",
    "           freq, a dictionary where freq[w] is the frequency of w.\n",
    "        \"\"\"\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "    # frequencies normalization and fitering\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        return freq\n",
    "\n",
    "    def summarize(self, text, n):\n",
    "        \"\"\"\n",
    "          Return a list of n sentences \n",
    "          which represent the summary of text.\n",
    "        \"\"\"\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)    \n",
    "        return [sents[j] for j in sents_idx]\n",
    "\n",
    "    def _rank(self, ranking, n):\n",
    "        \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTextFromURL(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_frequencies(tokenized_text,term):\n",
    "    freq = 0\n",
    "    for word in tokenized_text:\n",
    "        if word == term:\n",
    "            freq+=1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def summarizeURL(url, total_pars):\n",
    "    url_text = getTextFromURL(url).replace(u\"Â\", u\"\").replace(u\"â\", u\"\")\n",
    "\n",
    "    fs = FrequencySummarizer()\n",
    "    final_summary = fs.summarize(url_text.replace(\"\\n\",\" \"), total_pars)\n",
    "    return \" \".join(final_summary)\n",
    "'''\n",
    "#url = raw_input(\"Enter a URL\\n\")\n",
    "#stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "#print type(stopwords.words('english'))\n",
    "\n",
    "url = 'http://www.theedadvocate.org/?p=2241'\n",
    "url_text = getTextFromURL(url)\n",
    "fs = FrequencySummarizer()\n",
    "final_summary = fs.summarize(url_text.replace(\"\\n\",\" \"),7)\n",
    "final_summary = \" \".join(final_summary)\n",
    "#sents = sent_tokenize(final_summary)\n",
    "#word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "#print word_sent[0]\n",
    "tokenized_text = word_tokenize(final_summary)\n",
    "#print tokenized_text[0]\n",
    "freq = defaultdict(int)\n",
    "StopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "classified_text = st.tag(tokenized_text)\n",
    "#for term,tag in classified_text:\n",
    "#    print term, tag\n",
    "for term,tag in classified_text:\n",
    "    if tag == 'LOCATION' and term not in StopWords:\n",
    "        freq[term]=compute_frequencies(tokenized_text,term)\n",
    "#final_summary = summarizeURL(url, 5)\n",
    "#print final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'America']\n",
      "America\n"
     ]
    }
   ],
   "source": [
    "print freq.keys()\n",
    "#for key,value in freq.iteritems():\n",
    "#    print key,value\n",
    "\n",
    "sorted_freq = OrderedDict(sorted(freq.items(), key=operator.itemgetter(1), reverse = True))\n",
    "#print type(sorted_freq)\n",
    "#for key,value in sorted_freq.iteritems():\n",
    "#    print key,value\n",
    "    \n",
    "print sorted_freq.keys()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
