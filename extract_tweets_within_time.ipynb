{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "#import jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModelParser(tweepy.parsers.ModelParser):\n",
    "    def parse(self, method, payload):\n",
    "        result = super(MyModelParser, self).parse(method, payload)\n",
    "        result._payload = json.loads(payload)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_api():\n",
    "    ''' Function that loads the twitter API after authorizing the user. '''\n",
    "\n",
    "    consumer_key = 'qiOaRLVPldazXmYmF3IaIQw4L'\n",
    "    consumer_secret = 'FAzcCMF1UUyuNuSeddAA1nDJYPeXm6OhaCD084k1t3BZ0HleCY'\n",
    "    access_token = '717220472623071233-oeDgXLyYdqT92Mi06aaAGV7EtSExfKS'\n",
    "    access_secret = 'tVh6WAqVqJ5Pekb3skPON4OD46dyyBAIGOiWjPkZrtglC'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_search(api, query, max_tweets, max_id, since_id):\n",
    "    ''' Function that takes in a search string 'query', the maximum\n",
    "        number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "        tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets,\n",
    "                                    since_id=str(since_id),\n",
    "                                    max_id=str(max_id-1))\n",
    "#                                    geocode=geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "api = load_api()\n",
    "search_phrases = ['#edpolicy','#edreform', \n",
    "                     '#edreformtribe', '#edgap',\n",
    "                     '#literacy', '#teacherquality',\n",
    "                     '#urbaned', 'edadmin',\n",
    "                     '#edpolitics']\n",
    "maxTweets = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for search_phrase in search_phrases:\n",
    "    for tweets in tweepy.Cursor(api.search,search_phrase,\\\n",
    "                           lang=\"en\",\\\n",
    "                           since='2014-06-06',until='2017-06-06').items(maxTweets):\n",
    "        #print tweet.created_at, tweet.text\n",
    "    #print type(tweets)\n",
    "        tweets_data.append(tweets)\n",
    "    #csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_tweet_data = []\n",
    "for tweets in tweets_data:\n",
    "    json_tweet_data.append(tweets._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "486\n"
     ]
    }
   ],
   "source": [
    "print len(tweets_data)\n",
    "print len(json_tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford_ner_dir = '/home/dell/stanford-ner-2015-04-20/'\n",
    "eng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar)\n",
    "enc = lambda x: x.encode('latin1', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/dell/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "tweets_dict = defaultdict(list)\n",
    "for tweets in json_tweet_data:\n",
    "    if tweets.has_key('user'):\n",
    "        #print 'No user data - ignoring tweet.'\n",
    "        user = enc(tweets['user']['name'])\n",
    "        text = enc(tweets['text'])\n",
    "        #url = re.search(\"(?P<url>https?://[^\\s]+)\", text.lower())\n",
    "        expanded_url = tweets['entities']['urls']\n",
    "        #print expanded_url\n",
    "        if len(expanded_url)!=0 and expanded_url[0]['expanded_url']:\n",
    "            url = expanded_url[0]['expanded_url']\n",
    "            content = urllib.urlopen(url).read()\n",
    "            soup = BeautifulSoup(content)\n",
    "            tokenized_text = word_tokenize(soup.text)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            for term,tag in classified_text:\n",
    "                if tag == 'LOCATION':\n",
    "                    if term in tweets_dict.keys():\n",
    "                        tweets_dict[term].append(soup.text)\n",
    "                    else:\n",
    "                        tweets_dict[term]=[]\n",
    "                        tweets_dict[term].append(soup.text)\n",
    "        \n",
    "        else:\n",
    "            tokenized_text = word_tokenize(text)\n",
    "            classified_text = st.tag(tokenized_text)\n",
    "            for term,tag in classified_text:\n",
    "                if tag == 'LOCATION':\n",
    "                    if term in tweets_dict.keys():\n",
    "                        tweets_dict[term].append(text)\n",
    "                    else:\n",
    "                        tweets_dict[term]=[]\n",
    "                        tweets_dict[term].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive = defaultdict(list)\n",
    "negative = defaultdict(list)\n",
    "neutral = defaultdict(list)\n",
    "for keys in tweets_dict.keys():\n",
    "    for tweets in tweets_dict[keys]:\n",
    "        # pass tweet into TextBlob\n",
    "        tweet = TextBlob(tweets)\n",
    "\n",
    "        # output sentiment polarity\n",
    "        #print keys\n",
    "        #print tweet.sentiment.polarity\n",
    "\n",
    "        # determine if sentiment is positive, negative, or neutral\n",
    "        if tweet.sentiment.polarity < 0:\n",
    "            #sentiment = \"negative\"\n",
    "            if keys in negative.keys():\n",
    "                negative[keys].append(tweets)\n",
    "            else:\n",
    "                negative[keys]=[]\n",
    "                negative[keys].append(tweets)\n",
    "        elif tweet.sentiment.polarity == 0:\n",
    "            #sentiment = \"neutral\"\n",
    "            if keys in neutral.keys():\n",
    "                neutral[keys].append(tweets)\n",
    "            else:\n",
    "                neutral[keys]=[]\n",
    "                neutral[keys].append(tweets)\n",
    "        else:\n",
    "            #sentiment = \"positive\"\n",
    "            if keys in positive.keys():\n",
    "                positive[keys].append(tweets)\n",
    "            else:\n",
    "                positive[keys]=[]\n",
    "                positive[keys].append(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:280\n",
      "negative:177\n",
      "neutral:2\n",
      "[u'Canada', u'JFK', u'Mississippi', u'United', u'Arial', u'WA', u'Bridges', u'Paris', u'Arlington', u'N.W', u'Missouri', u'Helvetica', u'Minnesota', u'Cambodia', u'Illinois', u'Newark', u'Johannesburg', u'Hill', u'Fourth', u'Indiana', u'Louisiana', u'Worth', u'Texas', u'U.S.Leyla', u'Santiago', u'Columbia', u'Janeiro', u'Sacramento', u'Kansas', u'PHILADELPHIA', u'Connecticut', u'Montana', u'Flint', u'York', u'World', u'Atlanta', u'Normaalikoulu', u'Haven', u'Netherlands', u'COLORADO', u'Berlin', u'Ventures', u'Island', u'de', u'Lexington', u'Crystal', u'Voila', u'Alexandria', u'Topeka', u'Chaika', u'V.P', u'Cincinnati', u'River', u'Jersey', u'Goshorn', u'Bolivar', u'BrooklynMareesa', u'England', u'San', u'D.C', u'Facility', u'Baltimore', u'Fresno', u'India', u'Kong', u'Nottingham', u'Durham', u'Dakota', u'Seattle', u'Lonely', u'CrossRidge', u'Rochester', u'Ireland', u'District', u'Southampton', u'Lake', u'Sunnyside', u'Hospital', u'Raleway', u'School', u'Denver', u'France', u'Galvaniez', u'Sunshine', u'State', u'Spread', u'Sydney', u'Bloomington', u'Orange', u'New', u'Maryland', u'East', u'Norway', u'Vegas-area', u'Carolina', u'Paladin', u'Arizona', u'Efland', u'Korea', u'Rio', u'Pittsburgh', u'Sheffield', u'Michigan', u'ROC', u'Blacktown', u'Oregon', u'Nicosia', u'Hartford', u'Amazon', u'Kisida', u'China', u'Massachusetts', u'L.A.', u'Top', u'America', u'U.S.', u'Fort', u'Kingdom', u'Sellehun', u'Central', u'Tahoma', u'Shanghai-Shanghai', u'Forum', u'Leavenworth', u'Oklahoma', u'Finland', u'Florida', u'Reddit', u'D.V.M.', u'Silicon', u'Washington', u'Tucson', u'South', u'Sweden', u'Ohio', u'Taiwan', u'Leone', u'Bossier', u'Correctional', u'MA', u'AthletesNew', u'Boston', u'Elysian', u'PISA', u'Dead', u'Durban', u'UK', u'Zoomi', u'Japan', u'Huntington', u'Erie', u'Baldwin', u'Maine', u'Street', u'Brazil', u'Orleans', u'Hong', u'NC', u'City', u'Fairway', u'St.', u'Delaware', u'Asia', u',', u'Richmond', u'Rhode', u'Arkansas', u'Bethesda', u'Italy', u'Hollywood', u'Europe', u'Joensuu', u'Australia', u'North', u'MANOR', u'&', u'Pinterest', u'Southern', u'Russia', u'NY', u'Vegas', u'California', u'Loop', u'West', u'Memphis', u'Sun', u'Soundcloud', u'Brighton', u'Wynne', u'Ark', u'Georgia', u'Houston', u'Pennsylvania', u'Iowa', u'Convo', u'Detroit', u'Philadelphia', u'Cambridge', u'Switzerland', u'Crocker', u'Austin', u'Francisco', u'Road', u'Capitol', u'Beijing', u'Dallas', u'Colorado', u'Markel', u'Tenn.', u'Africa', u'Faerie', u'AZ', u'County', u'Sea', u'of', u'Ave', u'Ergotron', u'Nevada', u'Avenue', u'Toronto', u'Qatar', u'USD', u'Germany', u'USA', u'Chicago', u'Instructure', u'Shanghai', u'Valley', u'DC', u'L.L', u'Vancouver', u'US', u'Sierra', u'Hollygrove', u'L.A', u'Palatino', u'D.C.', u'States', u'Israel', u'U.S', u'TX', u'Mankato', u'Nashville', u'Utah', u'Virginia', u'Economic', u'Jordan', u'Uncategorized', u'Gradescope', u'Las', u'Zealand', u'VA', u'Vermont', u'Looney', u'Fields', u'Montserrat', u'Hawaii', u'Kentucky', u'Charles', u'Angeles', u'Nebraska', u'Martin', u'Glendale', u'Wisconsin', u'Quizizz', u'Pa.', u'Unified', u'DeVosJesse', u'Mexico', u'Singapore', u'Los', u'Reno', u'Porres', u'High', u'Middle', u'Garfield', u'Tennessee', u'SchoolInvestorsEnterprisesIn-depth', u'Earth', u'Lucida']\n"
     ]
    }
   ],
   "source": [
    "print \"positive\"+\":\"+str(len(positive.keys())) \n",
    "print \"negative\"+\":\"+str(len(negative.keys()))\n",
    "print \"neutral\"+\":\"+str(len(neutral.keys()))\n",
    "print positive.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
